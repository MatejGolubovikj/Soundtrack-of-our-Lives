{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5347aa32",
   "metadata": {},
   "source": [
    "# Age Aware Recommendation\n",
    "\n",
    "This notebook details the code of our age aware recommendation experiment, included in the paper \"Anonimous Authors, Soundtracks of Our Lives: How Age Influences Musical Preferences\" (the paper is currently going through a double-blind review process and therefore authors are omited). In cells such as this and inline comments, the code is documented for the purpose of exploitation and experiment reproducability.\n",
    "\n",
    "The code for this experiment is written in Python 3 utilizing the Jupyter Notebook format (https://jupyter-notebook.readthedocs.io). We also use the following exteral libraries:\n",
    "- \"Pandas\" (https://pandas.pydata.org/) - data wrangling library, \n",
    "- \"NumPy\" (https://numpy.org/) - linear algebra library,\n",
    "- \"LensKit\" (https://lenskit.org/) - recommender system library,\n",
    "\n",
    "as well as, the native python 3 libraties: \"collections\" (data structures for python), and \"random\" (random number generator).\n",
    "\n",
    "For the purpose of the experiment we use a sample of users, tracks and listening events (LEs) taken from the LFM2b dataset (\"http://www.cp.jku.at/datasets/LFM-2b/\"). The samples of users, tracks and LEs necessary for the experiment are included in the repository with this notebook. For more information on the sample, we point the reader to the aforementioned paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4c4d4",
   "metadata": {},
   "source": [
    "## 0. Results folder\n",
    "Before begining the experiment a folder needs to be created to store the results. We store path to the fold in the \"Results_folder\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_folder='Experiment_Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7c284",
   "metadata": {},
   "source": [
    "## 1. Necessary libraries and modules\n",
    "\n",
    "We import the necessary libraries and modules for the experiment. An overview of the libraries used can be found above. We use inline comments to document each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pandas and numpy libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#From LensKit load the necessary classes for bacth recommendations (\"batch\"), top-n recommendation evaluation (\"topn\")\n",
    "from lenskit import batch, topn\n",
    "\n",
    "#From LensKit load the User KNN algorithm class (\"user_knn\").\n",
    "from lenskit.algorithms import user_knn\n",
    "\n",
    "#From LensKit load a warper for Top N recommendations (\"TopN\")\n",
    "#The warper will generate a ranked list of recommendations using the provided algorithm\n",
    "from lenskit.algorithms.ranking import TopN\n",
    "\n",
    "#From LensKit load a selector for unrated (unseen) items, we use this to select candidate items\n",
    "from lenskit.algorithms.basic import UnratedItemCandidateSelector\n",
    "\n",
    "#From the base python pacjages we load a priority queue class ('deque') and a random number generator ('random').\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instruction for pandas not to give chained assignment warnings (this is for clearer output during execution)\n",
    "#All instances whith chained assignment warnings were checked for correctness prior to performing the experiment\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80648484",
   "metadata": {},
   "source": [
    "## 2. Importing and preprocessing the necessary data\n",
    "\n",
    "We import the necessary data for the experiment. For the purpose of the experiment we use a sample of users, tracks and listening events (LEs) taken from the LFM2b dataset (\"http://www.cp.jku.at/datasets/LFM-2b/\"). The samples of users, tracks and LEs necessary for the experiment are included in the repository with this notebook. For more information on the sample, we point the reader to the aforementioned paper. \n",
    "\n",
    "After the data we select only those user with >20 intearections to be included in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the ratings data and remane columns to fit the format used in LensKit algorithms (user, item, rating)\n",
    "ratings=pd.read_parquet(\"Data/Experiment_Data/full_training_subset_of_select_tracks_experiment.parquet\")\n",
    "ratings.columns=['user', 'item', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the user age and track artist data\n",
    "user_age_index_df=pd.read_parquet(\"Data/Experiment_Data/user_age_index_basic_for_experiment_updated.parquet\")\n",
    "user_age_index_df=user_age_index_df.reset_index()\n",
    "artists_index_df=pd.read_parquet(\"Data/Experiment_Data/tracks_artist_index_for_experiment.parquet\")\n",
    "\n",
    "#Convert the user age and artist data into a lookup dictonary\n",
    "user_age_index=dict(zip(list(user_age_index_df['user']), list(user_age_index_df['user_age'])))\n",
    "artists_index=dict(zip(list(artists_index_df['item']), list(artists_index_df['artist'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only users who have listened to more than 20 tracks in our sample\n",
    "user_counts=ratings.groupby('user').count().reset_index()\n",
    "users_admited=user_counts[user_counts['item']>20]\n",
    "users_admited=pd.DataFrame({'user':users_admited.user.unique()})\n",
    "ratings=ratings.merge(users_admited,how='inner',on='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a9a7d",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "For the purpose of the experiment we define a number of utility functions. Primaraly, a function needed to adjust the diversity of the top n lists of each user for given % (to test our diversity adjustment hypothesis) and a function to split the data into folds where each user is equally represented in all folds.\n",
    "\n",
    "These utility functions are:\n",
    "\n",
    "**adjust_divesities \n",
    "(users, diversity, TopN_lists_persanolized, n,k):** \n",
    "\n",
    "- Adjusts the diversity of the top n lists of each user. It takes as **input** (i) a list of presonalized recomendations (\"TopN_lists_persanolized\"),  (ii) a set of users (\"users\"), (iii) a desired diversity adjustment % (\"diversity\") and (iv) desired top n list size ('k'). \n",
    "\n",
    "- **Returns** a list of top n list for each adjusted for diversity of the desired length 'k' as well as the average intra-list artist diversity of the lists after adjustment\n",
    "\n",
    "\n",
    "**split_by_users(ratings, folds=5):**\n",
    "\n",
    "- For a **given** (i) ratings dataframe (\"ratings\") and (ii) number of folds (\"folds\"),\n",
    "- **returns** the orignal ratings dataframe split into the desired number of folds per user (the interactions of each user split equally among all folds).\n",
    "\n",
    "\n",
    "**generate_recommendations(user_set, algorithm, n=100):** \n",
    "\n",
    "- For a **given** (i) set of users (\"user_set\"), (ii) recommendation algorithm (\"algorithm\") and (iii) a desired number of recommendations (\"n\"), \n",
    "- it **returns** a list of \"n\" recommended tracks for each user based on the given algorithm.\n",
    "\n",
    "\n",
    "\n",
    "**evaluate_recomendations(recomendations, truth, k=100):**\n",
    "\n",
    "- For **given** (i) recomendation lists per user (\"recomendations\"), (ii) true interaction per user (\"truth\"), and the length of each recomended list (\"k\")\n",
    "\n",
    "- it **returns** evalutations of the recomendations for each user using the following metrics:ndcg, precision, recall, hitrate (calculate these methric using the function provided by LensKit).\n",
    "\n",
    " We use inline comments to document the procedure of each function further.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ajdust_diversity_user(userID, diversity, topN_list, n=100,k=50):\n",
    "    \n",
    "    #Convert float probability to a whole number format (percentages)\n",
    "    similar_track_removal_probability=int(np.floor(diversity*100))\n",
    "    \n",
    "    #Define a list to hold the artists that are already present in the topN_list,\n",
    "    #and a list to hold the indices of the tracks we should remove to adjust the diversity\n",
    "    artist_already_pressent=[]\n",
    "    removed_tracks_indices=[]\n",
    "    \n",
    "    #For i in the range of 0 to the length of the Top n list (the first 'top' track to the last (lowest rated) track)\n",
    "    for i in range(0,len(topN_list)):\n",
    "        \n",
    "        #Find the next track in the list, it's rank and it's artist\n",
    "        candidate=topN_list.iloc[i]\n",
    "        item=int(candidate['item'])\n",
    "        rank=int(candidate['rank'])\n",
    "        item_artist=artists_index[item]\n",
    "        \n",
    "        #If the artist has already been seen in the list, remove it from the list with the probability of 'similar_track_removal_probability'\n",
    "        if (item_artist in artist_already_pressent):\n",
    "            removal_chance=random.randrange(1,100)\n",
    "            if(removal_chance<similar_track_removal_probability):\n",
    "                removed_tracks_indices.append(i)\n",
    "        #Else, if the artist has not yet been seen in the recommendation list, add it to the list of already seen artists\n",
    "        else:\n",
    "            artist_already_pressent.append(item_artist)\n",
    "\n",
    "        #If we have reached the desired length of the Top n list, held in the variable 'k' , break the loop\n",
    "        if i == k:\n",
    "            break\n",
    "    \n",
    "    #Remove the tracks that have been flagged for removal and reconstruct the (ranks in the) topN list\n",
    "    topN_list_final=topN_list.drop(removed_tracks_indices).reset_index(drop=True)\n",
    "    topN_list_final['rank']=list(range(1,len(topN_list_final)+1))\n",
    "    \n",
    "    #Select the top k tracks in the list, where k is the desired length of the recomendation list\n",
    "    topN_list_final=topN_list_final.head(k)\n",
    "    \n",
    "    #If a list is pressent in the topN_list, return the list and calculation of the interalistic diversity of the list\n",
    "    #Inra-list diversity in our case is defined as\n",
    "    #the number of artists in the list divided by the number of recommendations in the topN_list\n",
    "    if len(topN_list_final)>0:\n",
    "        return len(artist_already_pressent)/len(topN_list_final), topN_list_final\n",
    "    \n",
    "    #In the case of no list, return 0 intra-list diversity and the list\n",
    "    else:\n",
    "        return 0, topN_list_final\n",
    "\n",
    "\n",
    "#Adjust the diversities each user by a given % (\"diversity\" paramater)\n",
    "def adjust_divesities(users, diversity, TopN_lists_persanolized, n=100,k=50):\n",
    "    \n",
    "    #Set vraiables to record the final lists and intralistic diversity of each user \n",
    "    final_list=None\n",
    "    set_list=True\n",
    "    diversities=[]\n",
    "    \n",
    "    #users_processed=0\n",
    "    \n",
    "    #For each user \"user\"\n",
    "    for user in users:\n",
    "        \n",
    "        #Select the list of recomendations for the user\n",
    "        user_list=TopN_lists_persanolized[TopN_lists_persanolized['user']==user]\n",
    "        user_list=user_list.reset_index(drop=True)\n",
    "        \n",
    "        #Adjust the diversity of the user's list by the % defined in the \"diversity\" paramater,\n",
    "        #and retrieve the intra-list diversity of the final list\n",
    "        \n",
    "        intalistic_diversitiy, user_adujusted_list=ajdust_diversity_user(user, diversity, user_list, n=n,k=k)\n",
    "        \n",
    "        #Record the intralistic diversity of the users adjuste list\n",
    "        diversities.append(intalistic_diversitiy)\n",
    "        if set_list:\n",
    "            final_list=user_adujusted_list\n",
    "            set_list=False\n",
    "        else:\n",
    "            final_list=final_list.append(user_adujusted_list)\n",
    "    \n",
    "    #Filter the users with zero recommendations \n",
    "    diversities=list(filter(lambda x: x != 0, diversities))\n",
    "    #Return the average inta-list diversity of the users and the adjusted lists of recomendations\n",
    "    return sum(diversities)/len(diversities), final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_DF(series):\n",
    "    return pd.DataFrame(series).T\n",
    "\n",
    "def generate_recommendations(user_set, algorithm, n=100):\n",
    "    \n",
    "    recommendations = batch.recommend(algorithm, user_set, n ,n_jobs=25)\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "def evaluate_recomendations(recomendations, truth, k=100):\n",
    "    \n",
    "    analysis = topn.RecListAnalysis()\n",
    "    analysis.add_metric(topn.ndcg,k=k)\n",
    "    analysis.add_metric(topn.precision,k=k)\n",
    "    analysis.add_metric(topn.recall,k=k)\n",
    "    analysis.add_metric(topn.hit)\n",
    "    results = analysis.compute(recomendations, truth)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee96156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_split(lenght, folds=5):\n",
    "    \n",
    "    adjustment_for_remainder_after_split=lenght % folds \n",
    "    split_length=int((lenght-adjustment_for_remainder_after_split)/folds)\n",
    "    \n",
    "    \n",
    "    split=[]\n",
    "    \n",
    "    \n",
    "    for i in range(1,6):\n",
    "        if adjustment_for_remainder_after_split >0:\n",
    "            split=split+[i]*(split_length+1)\n",
    "            adjustment_for_remainder_after_split=adjustment_for_remainder_after_split - 1\n",
    "        else:\n",
    "            split=split+[i]*(split_length)\n",
    "    \n",
    "    return split\n",
    "\n",
    "def split_by_users(ratings, folds=5):\n",
    "    \n",
    "    #Sampling 100% of the dataset just returnes the datased shuffled \n",
    "    ratings=ratings.sample(frac=1)\n",
    "    splited_ratings=None\n",
    "    \n",
    "    set_inital=True\n",
    "    for _,user in ratings.groupby('user', sort=False):\n",
    "        user['split']=generate_single_split(len(user),folds=folds)\n",
    "        \n",
    "        if set_inital:\n",
    "            splited_ratings=user\n",
    "            set_inital=False\n",
    "        else:\n",
    "            splited_ratings=splited_ratings.append(user)\n",
    "    return splited_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e22221",
   "metadata": {},
   "source": [
    "## 4. Experiment Procedure\n",
    "\n",
    "This section details the procedure of the experiment. We first split the data into 5 folds for cross-validation, and then train and cross-validate a user-based KNN recommendation algorithm for each combination of paramaters:\n",
    "\n",
    "- Age range: 10-20 16-26, 26-36, 36-46, 46-56, 49-59, 49-55, 55-61 and 10-64 (all ages, used as a baseline).\n",
    "- Number of neighbours: 6, 8, 12, 18, 24, 36, 50, 60, 70, 100, 110, 120, 150\n",
    "- Diversity adjustment: 0 (no adjustement), 0.2, 0.4, 0.6\n",
    "\n",
    "For each algorithm we geneate a list of 10 recommendations and evaluate NDCG@10, Precision@10, Recall@10 and HitRate@10 (Recall and HitRate are not used in the final paper).\n",
    "\n",
    "The final results of the procedure are expoted as a .csv file and stored in the \"Results_folder\". We use inline comments to document the steps further in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c89bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the ratings into 5 folds per User\n",
    "\n",
    "ratings=split_by_users(ratings, folds=5)\n",
    "\n",
    "#EXTRA: Save and load ratings with split if needed\n",
    "#ratings.to_parquet(\"Data/Experiment_Data/Ratings_with_Split.parquet\")\n",
    "#ratings=pd.read_parquet(\"Data/Experiment_Data/Ratings_with_Split.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossvalidate sliding window\n",
    "\n",
    "#Define the set of paramates and age ranges which we will test on\n",
    "age_ranges=[(10,64),(10,20),(16,26),(26,36),(36,46),(46,56),(49,59),(49,55),(55,61)] \n",
    "neighbours_set=[6,8,12,18,24,36,50,60,70,100,110,120,150]\n",
    "diversities=[0,0.2,0.4,0.6]\n",
    "#To acomodate for testing the performance in multiple list lengths define a set of list lengths to be tested and the number of recommendations n before divesity adjustment\n",
    "#In our case we only test for a list length of 10, and initial list length before divesity adjustment of n=50\n",
    "list_lengths=[10]\n",
    "\n",
    "n=50\n",
    "\n",
    "\n",
    "results=None\n",
    "set_results=True\n",
    "cv_iter=0\n",
    "\n",
    "#For each of the 5 folds (CV iterations) \"cv_iter\"\n",
    "for cv_iter in range(1,5+1):\n",
    "    \n",
    "    #Select the train and test split for the itration\n",
    "    test=ratings[ratings['split']==cv_iter]\n",
    "    train=ratings.drop(test.index)\n",
    "    \n",
    "    #Drop the split column (the column which indicates to which split the rating belong)\n",
    "    test=test.drop('split', axis=1)\n",
    "    train=train.drop('split', axis=1)\n",
    "    \n",
    "    print(\"Testing in CV iteration:\",cv_iter)\n",
    "    \n",
    "    #For each configuration of the number of neighbours \"neighbours\"\n",
    "    for neighbours in neighbours_set:\n",
    "        print(\"Traning recommender with\",neighbours,\"neighbours\") #, save_nbrs=100\n",
    "        \n",
    "        #Buld a KNN recommender with the selected number of neighbours\n",
    "        predictor = user_knn.UserUser(neighbours,min_nbrs=neighbours,center=False,feedback='implicit',use_ratings=False)#\n",
    "        Unseen_item_selector = UnratedItemCandidateSelector()\n",
    "        recommender = TopN(predictor, Unseen_item_selector)    \n",
    "        predictor.fit(train)\n",
    "        Unseen_item_selector.fit(train)\n",
    "        \n",
    "        #Generate a large list of recomendations for all users\n",
    "        recomendations_all=generate_recommendations(test.user.unique(),recommender,n=n)\n",
    "        \n",
    "        #For each of the recomendation list length \"k\" (in the final paper this is only 10)\n",
    "        for k in list_lengths:\n",
    "            print(\"Testing at\",k,\"recommendations\")\n",
    "            \n",
    "            #For each diversity configuration \"diversity\",\n",
    "            for diversity in diversities:  \n",
    "                \n",
    "                #and for each age range\n",
    "                for age_range_start,age_range_end in age_ranges:\n",
    "\n",
    "                    #Find the users in the age range\n",
    "                    age_range_str=str(age_range_start)+'-'+str(age_range_end)\n",
    "                    users_in_age_range=user_age_index_df[(user_age_index_df['user_age']>=(age_range_start)) & \n",
    "                                            (user_age_index_df['user_age']<=(age_range_end))]                \n",
    "\n",
    "                    info= \"age range \"+age_range_str+\" with \"+str(neighbours)+\" neighbours and \" + str(diversity) + \" diversity\" \n",
    "                    print('Processing',info,\"in itteration\",cv_iter)\n",
    "                    \n",
    "                    #Find the list of recommendations in the age range and the true list of consumed items in the age range\n",
    "                    truth_in_age_range=test.merge(users_in_age_range[['user']], how='inner', on='user')\n",
    "                    recs_in_age_range = recomendations_all.merge(users_in_age_range[['user']], how='inner', on='user')\n",
    "\n",
    "                    #Adjust the diversity based on the diversity paramater, generate a list of k recommendations, and calculate the intralist diversity\n",
    "                    Intralist_diversity_avg, recs_in_age_range = adjust_divesities(users_in_age_range.user.unique(), diversity, \n",
    "                                                          recs_in_age_range, n=n,k=k)\n",
    "                    \n",
    "                    #Evaluate the recomendations\n",
    "                    results_i=evaluate_recomendations(recs_in_age_range, truth_in_age_range, k=k)\n",
    "                    \n",
    "                    #*Export per user evaluations in this configuration and age range \n",
    "                    filename='User_Results_'+age_range_str+'_'+str(neighbours)+'_neig_'+str(int(10*diversity))+\"_diver_CViter_\"+str(cv_iter)+'.csv'\n",
    "                    results_i=results_i.reset_index()\n",
    "                    results_i.to_csv(Results_folder+CV_iters/'+filename, index=False)\n",
    "                    \n",
    "                    #Average the results in this configuration and age range over all users\n",
    "                    results_i=results_i[[\"ndcg\",\"precision\",\"recall\",\"hit\"]].mean()\n",
    "                    \n",
    "                    #Convert the results to a dataframe and add them to the overall table of result\n",
    "                    results_i=convert_to_DF(results_i)\n",
    "                    results_i['List_Len']=[k]*len(results_i)\n",
    "                    results_i['Neighbours']=[neighbours]*len(results_i)\n",
    "                    results_i['Diversity_adjustment']=[diversity]*len(results_i)\n",
    "                    results_i['Intralist_Diversity_calculated']=[Intralist_diversity_avg]*len(results_i)\n",
    "                    results_i['Age_range']=[age_range_str]*len(results_i)\n",
    "                    results_i['CV_iter']=[cv_iter]*len(results_i)\n",
    "                    if set_results:\n",
    "                        results=results_i\n",
    "                        set_results=False\n",
    "                    else:\n",
    "                        results=results.append(results_i)\n",
    "\n",
    "                    print('Final Evaluation for',k, 'recomendations in', info, \", with Inta-user\",Intralist_diversity_avg)\n",
    "                    print(results_i)\n",
    "\n",
    "    #Export intermediate results per fold as csv\n",
    "    sifix=\"_at_fold_\"+str(cv_iter)\n",
    "    results.to_parquet(Results_folder+\"Results_Cross_validationNew_5_10_2\"+sifix+\".parquet\")\n",
    "\n",
    "#Export final results as csv\n",
    "results.to_parquet(Results_folder+\"Results_Cross_validationNew_5_10_2_Final.parquet\")    \n",
    "results.head()                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84322cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For each age group, configuration (neighbours, diversity adjustment) and list length, get the mean results.\n",
    "results_means_all=results.groupby(['Age_range','Neighbours','Diversity_adjustment','List_Len']).mean().reset_index()\n",
    "\n",
    "#For each list length, print the 4 best performing configurations of each age range and merge all of them in one table.\n",
    "print_l=True\n",
    "list_lengths=[10]\n",
    "add=None\n",
    "set_add=True\n",
    "for list_len in list_lengths:\n",
    "    \n",
    "    results_means=results_means_all[results_means_all['List_Len']==list_len]\n",
    "    for age_r in results.Age_range.unique():\n",
    "        print(\"Age Range\",age_r)\n",
    "        results_age_r=results_means[results_means['Age_range']==age_r].sort_values(by='ndcg',ascending=False).head(4)\n",
    "        print(results_age_r)\n",
    "\n",
    "        if set_add:\n",
    "            add=results_age_r\n",
    "            set_add=False\n",
    "        else:\n",
    "            add=add.append(results_age_r)\n",
    "\n",
    "#Export the table of best performing results into csv format.\n",
    "filename=Results_folder+\"Results_Cross_validation_AllAges_Summarized.csv\"\n",
    "add.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
